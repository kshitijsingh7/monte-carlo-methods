{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a62022-c0e7-4bfd-908e-2232de2f1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa66685-b377-4c05-9fd0-31a524b150da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a550f5d1-7d3d-4233-b571-0d16b6ee641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_during_training = False   \n",
    "render_during_simulation = True  \n",
    "use_slippery = False             # set True for stochastic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe05631b-cc1a-44bd-a6e5-31777d794331",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode_train = \"human\" if render_during_training else None\n",
    "render_mode_sim = \"human\" if render_during_simulation else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e74a9b-f79c-48c6-9794-8af56f1c9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make(\"FrozenLake-v1\", is_slippery=use_slippery, render_mode=render_mode_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ecb738c-c38b-4c07-aea9-c60cb2116e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "gamma = 0.95\n",
    "epsilon = 0.3\n",
    "\n",
    "Returns = defaultdict(float)\n",
    "Num = defaultdict(int)\n",
    "Q = defaultdict(float)\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b03f41a-55d8-43cf-bfb8-6749b0d46301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    n_actions = env_train.action_space.n\n",
    "    action_values = [Q[(state, a)] for a in range(n_actions)]\n",
    "    best_action = np.argmax(action_values)\n",
    "    policy = np.ones(n_actions) * (epsilon / n_actions)\n",
    "    policy[best_action] += 1.0 - epsilon\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15bdf4-c746-4a52-adf5-4ebe6cb9d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env_train.reset()[0]\n",
    "    episode_data = []\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        if render_during_training:\n",
    "            env_train.render()\n",
    "\n",
    "        policy = epsilon_greedy_policy(state)\n",
    "        action = np.random.choice(np.arange(env_train.action_space.n), p=policy)\n",
    "        next_state, reward, done, truncated, _ = env_train.step(action)\n",
    "        episode_data.append((state, action, reward))\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # monte carlo update\n",
    "    G = 0\n",
    "    for t in reversed(range(len(episode_data))):\n",
    "        s_t, a_t, r_t = episode_data[t]\n",
    "        G = gamma * G + r_t\n",
    "        if not any((x[0] == s_t and x[1] == a_t) for x in episode_data[:t]):\n",
    "            Returns[(s_t, a_t)] += G\n",
    "            Num[(s_t, a_t)] += 1\n",
    "            Q[(s_t, a_t)] = Returns[(s_t, a_t)] / Num[(s_t, a_t)]\n",
    "\n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832375bf-7efe-4c4e-97ce-547427ef4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "for state in range(env_train.observation_space.n):\n",
    "    actions = [Q[(state, a)] for a in range(env_train.action_space.n)]\n",
    "    policy[state] = np.argmax(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba066159-2f4d-4b1c-9a58-fcfdcac26476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(policy, env):\n",
    "    arrows = ['←', '↓', '→', '↑']\n",
    "    desc = env.unwrapped.desc.astype(str)\n",
    "    size = env.unwrapped.nrow\n",
    "\n",
    "    print(\"\\nLearned Policy with Map:\")\n",
    "    for i in range(size):\n",
    "        row = ''\n",
    "        for j in range(size):\n",
    "            state = i * size + j\n",
    "            tile = desc[i][j]\n",
    "\n",
    "            if tile == 'H' or tile == 'G':\n",
    "                row += f'{tile} '\n",
    "            elif tile == 'S':\n",
    "                row += f'[{arrows[policy[state]]}]'\n",
    "            else:\n",
    "                row += f'{arrows[policy[state]]} '\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed279d41-ba0c-436a-acbe-e0eabda078cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(policy, env_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efc0b5-8a87-46ee-adab-0269298b4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_run(policy, delay=0.7):\n",
    "    env_sim = gym.make(\"FrozenLake-v1\", is_slippery=use_slippery, render_mode=render_mode_sim)    \n",
    "    state = env_sim.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    print(\"\\nSimulation Run:\")\n",
    "    time.sleep(2)\n",
    "    while not done:\n",
    "        time.sleep(delay)\n",
    "        action = policy[state]\n",
    "        state, reward, done, truncated, _ = env_sim.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Total Reward:\", total_reward)\n",
    "    time.sleep(3)\n",
    "    env_sim.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9b70f-59c2-4d2a-afdb-3f4121968722",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_run(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8b759-bb3c-42a2-b2bb-c8398b53c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Reward per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46518446-5e73-4ce0-9071-9c653745a5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
